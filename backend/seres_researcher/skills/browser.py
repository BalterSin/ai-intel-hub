from seres_researcher.utils.workers import WorkerPool

from ..actions.utils import stream_output
from ..actions.web_scraping import scrape_urls
from ..web_scraper.utils import get_image_hash


class BrowserManager:
    """ç®¡ç†Research Agentçš„ä¸Šä¸‹æ–‡ã€‚"""

    def __init__(self, researcher):
        self.researcher = researcher
        self.worker_pool = WorkerPool(researcher.cfg.max_scraper_workers)

    async def browse_urls(self, urls: list[str]) -> list[dict]:
        """
        ä»ä¸€ç»„URLä¸­æŠ“å–å†…å®¹ã€‚

        å‚æ•°ï¼š
            urls (list[str]): è¦æŠ“å–çš„URLåˆ—è¡¨ã€‚

        è¿”å›ï¼š
            list[dict]: æŠ“å–å†…å®¹çš„ç»“æœåˆ—è¡¨ã€‚
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "scraping_urls",
                f"ğŸŒ æ­£åœ¨ä» {len(urls)} ä¸ªURLä¸­æŠ“å–å†…å®¹...",
                self.researcher.websocket,
            )

        scraped_content, images = await scrape_urls(
            urls, self.researcher.cfg, self.worker_pool
        )
        self.researcher.add_research_sources(scraped_content)
        new_images = self.select_top_images(images, k=4)  # é€‰æ‹©å‰4å¼ å›¾ç‰‡
        self.researcher.add_research_images(new_images)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "scraping_content",
                f"ğŸ“„ å·²æŠ“å– {len(scraped_content)} é¡µå†…å®¹",
                self.researcher.websocket,
            )
            await stream_output(
                "logs",
                "scraping_images",
                f"ğŸ–¼ï¸ ä» {len(images)} å¼ å›¾ç‰‡ä¸­é€‰æ‹©äº† {len(new_images)} å¼ æ–°å›¾ç‰‡",
                self.researcher.websocket,
                True,
                new_images,
            )
            await stream_output(
                "logs",
                "scraping_complete",
                f"ğŸŒ æŠ“å–å®Œæˆ",
                self.researcher.websocket,
            )

        return scraped_content

    def select_top_images(self, images: list[dict], k: int = 2) -> list[str]:
        """
        æ ¹æ®å›¾ç‰‡å†…å®¹é€‰æ‹©æœ€ç›¸å…³çš„å›¾ç‰‡å¹¶å»é™¤é‡å¤é¡¹ã€‚

        å‚æ•°ï¼š
            images (list[dict]): åŒ…å« 'url' å’Œ 'score' é”®çš„å›¾ç‰‡å­—å…¸åˆ—è¡¨ã€‚
            k (int): å¦‚æœæ²¡æœ‰é«˜åˆ†å›¾ç‰‡æ—¶è¦é€‰æ‹©çš„å›¾ç‰‡æ•°é‡ã€‚

        è¿”å›ï¼š
            list[str]: é€‰å®šçš„å›¾ç‰‡URLåˆ—è¡¨ã€‚
        """
        unique_images = []
        seen_hashes = set()
        current_research_images = self.researcher.get_research_images()

        # æŒ‰å›¾ç‰‡è¯„åˆ†é™åºå¤„ç†å›¾ç‰‡
        for img in sorted(images, key=lambda im: im["score"], reverse=True):
            img_hash = get_image_hash(img['url'])
            if (
                img_hash
                and img_hash not in seen_hashes
                and img['url'] not in current_research_images
            ):
                seen_hashes.add(img_hash)
                unique_images.append(img["url"])

                if len(unique_images) == k:
                    break

        return unique_images
