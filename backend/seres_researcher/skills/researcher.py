import asyncio
import random
import logging
import os
from ..actions.utils import stream_output
from ..actions.query_processing import plan_research_outline, get_search_results
from ..document import DocumentLoader, OnlineDocumentLoader, LangChainDocumentLoader
from ..utils.enum import ReportSource
from ..utils.logging_config import get_json_handler


class ResearchConductor:
    """ç®¡ç†å¹¶åè°ƒç ”ç©¶è¿‡ç¨‹ã€‚"""

    def __init__(self, researcher):
        self.researcher = researcher
        self.logger = logging.getLogger('research')
        self.json_handler = get_json_handler()

    async def plan_research(self, query, query_domains=None):
        self.logger.info(f"ä¸ºä»¥ä¸‹Queryè§„åˆ’ç ”ç©¶ï¼š{query}")
        if query_domains:
            self.logger.info(f"Queryé¢†åŸŸï¼š{query_domains}")
        
        await stream_output(
            "logs",
            "planning_research",
            f"ğŸŒ æµè§ˆç½‘ç»œä»¥äº†è§£æ›´å¤šå…³äºä»»åŠ¡çš„ä¿¡æ¯ï¼š{query}...",
            self.researcher.websocket,
        )

        search_results = await get_search_results(query, self.researcher.retrievers[0], query_domains)
        self.logger.info(f"åˆå§‹æœç´¢ç»“æœå·²è·å¾—ï¼š{len(search_results)} æ¡ç»“æœ")

        await stream_output(
            "logs",
            "planning_research",
            f"ğŸ¤” åˆ¶å®šç ”ç©¶è®¡åˆ’å’Œä»»åŠ¡æ‹†è§£...",
            self.researcher.websocket,
        )

        outline = await plan_research_outline(
            query=query,
            search_results=search_results,
            agent_role_prompt=self.researcher.role,
            cfg=self.researcher.cfg,
            parent_query=self.researcher.parent_query,
            report_type=self.researcher.report_type,
            cost_callback=self.researcher.add_costs,
            **self.researcher.kwargs
        )
        self.logger.info(f"ç ”ç©¶å¤§çº²å·²è§„åˆ’ï¼š{outline}")
        return outline

    async def conduct_research(self):
        """è¿è¡Œ Seres ç ”ç©¶è€…è¿›è¡Œç ”ç©¶"""
        if self.json_handler:
            self.json_handler.update_content("query", self.researcher.query)
        
        self.logger.info(f"å¼€å§‹å¯¹ä»¥ä¸‹Queryè¿›è¡Œç ”ç©¶ï¼š{self.researcher.query}")
        
        # æ¯æ¬¡ç ”ç©¶ä»»åŠ¡å¼€å§‹æ—¶é‡ç½® visited_urls å’Œ source_urls
        self.researcher.visited_urls.clear()
        research_data = []

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "starting_research",
                f"ğŸ” å¼€å§‹å¯¹ '{self.researcher.query}' è¿›è¡Œç ”ç©¶ä»»åŠ¡...",
                self.researcher.websocket,
            )
            await stream_output(
                "logs",
                "agent_generated",
                self.researcher.agent,
                self.researcher.websocket
            )

        # æ ¹æ®ä»¥ä¸‹æºç±»å‹ç ”ç©¶ç›¸å…³ä¿¡æ¯
        if self.researcher.source_urls:
            self.logger.info("ä½¿ç”¨æä¾›çš„æºURL")
            research_data = await self._get_context_by_urls(self.researcher.source_urls)
            if research_data and len(research_data) == 0 and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "answering_from_memory",
                    f"ğŸ§ åœ¨æä¾›çš„èµ„æºä¸­æ‰¾ä¸åˆ°ç›¸å…³å†…å®¹...",
                    self.researcher.websocket,
                )
            if self.researcher.complement_source_urls:
                self.logger.info("é€šè¿‡ç½‘ç»œæœç´¢è¡¥å……")
                additional_research = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
                research_data += ' '.join(additional_research)

        elif self.researcher.report_source == ReportSource.Web.value:
            self.logger.info("ä½¿ç”¨ç½‘ç»œæœç´¢")
            research_data = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)

        elif self.researcher.report_source == ReportSource.Local.value:
            self.logger.info("ä½¿ç”¨æœ¬åœ°æœç´¢")
            document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            self.logger.info(f"å·²åŠ è½½ {len(document_data)} ä»½æ–‡æ¡£")
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)

            research_data = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)

        # æ··åˆæœç´¢ï¼ŒåŒ…æ‹¬æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œèµ„æº
        elif self.researcher.report_source == ReportSource.Hybrid.value:
            if self.researcher.document_urls:
                document_data = await OnlineDocumentLoader(self.researcher.document_urls).load()
            else:
                document_data = await DocumentLoader(self.researcher.cfg.doc_path).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(document_data)
            docs_context = await self._get_context_by_web_search(self.researcher.query, document_data, self.researcher.query_domains)
            web_context = await self._get_context_by_web_search(self.researcher.query, [], self.researcher.query_domains)
            research_data = self.researcher.prompt_family.join_local_web_documents(docs_context, web_context)

            
        elif self.researcher.report_source == ReportSource.LangChainDocuments.value:
            langchain_documents_data = await LangChainDocumentLoader(
                self.researcher.documents
            ).load()
            if self.researcher.vector_store:
                self.researcher.vector_store.load(langchain_documents_data)
            research_data = await self._get_context_by_web_search(
                self.researcher.query, langchain_documents_data, self.researcher.query_domains
            )

        elif self.researcher.report_source == ReportSource.LangChainVectorStore.value:
            research_data = await self._get_context_by_vectorstore(self.researcher.query, self.researcher.vector_store_filter)

        # å¯¹æ¥æºè¿›è¡Œæ’åºå’Œæ•´ç†
        self.researcher.context = research_data
        if self.researcher.cfg.curate_sources:
            self.logger.info("æ•´ç†æ¥æº")
            self.researcher.context = await self.researcher.source_curator.curate_sources(research_data)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "research_step_finalized",
                f"ç ”ç©¶æ­¥éª¤å·²å®Œæˆã€‚\nğŸ’¸ æ€»ç ”ç©¶æˆæœ¬ï¼š${self.researcher.get_costs()}",
                self.researcher.websocket,
            )
            if self.json_handler:
                self.json_handler.update_content("costs", self.researcher.get_costs())
                self.json_handler.update_content("context", self.researcher.context)

        self.logger.info(f"ç ”ç©¶å·²å®Œæˆã€‚ä¸Šä¸‹æ–‡å¤§å°ï¼š{len(str(self.researcher.context))}")
        return self.researcher.context

    async def _get_context_by_urls(self, urls):
        """ä»ç»™å®šçš„URLä¸­æŠ“å–å¹¶å‹ç¼©ä¸Šä¸‹æ–‡"""
        self.logger.info(f"ä»ä»¥ä¸‹URLè·å–ä¸Šä¸‹æ–‡ï¼š{urls}")
        
        new_search_urls = await self._get_new_urls(urls)
        self.logger.info(f"éœ€è¦å¤„ç†çš„æ–°URLï¼š{new_search_urls}")

        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)
        self.logger.info(f"å·²ä» {len(scraped_content)} ä¸ªURLä¸­æŠ“å–å†…å®¹")

        if self.researcher.vector_store:
            self.logger.info("å°†å†…å®¹åŠ è½½åˆ°VectorStoreä¸­")
            self.researcher.vector_store.load(scraped_content)

        context = await self.researcher.context_manager.get_similar_content_by_query(
            self.researcher.query, scraped_content
        )
        return context

    # å…¶ä»–æ–¹æ³•åŒæ ·æ·»åŠ æ—¥å¿—...

    async def _get_context_by_vectorstore(self, query, filter: dict | None = None):
        """
        é€šè¿‡æœç´¢VectorStoreç”Ÿæˆç ”ç©¶ä»»åŠ¡çš„ä¸Šä¸‹æ–‡
        è¿”å›ï¼š
            context: ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹VectorStoreæœç´¢ï¼ŒQueryï¼š{query}")
        context = []
        # ç”Ÿæˆå­æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸå§‹Query
        sub_queries = await self.plan_research(query)
        # å¦‚æœè¿™ä¸æ˜¯å­ç ”ç©¶è€…çš„ä¸€éƒ¨åˆ†ï¼Œæ·»åŠ åŸå§‹Queryä»¥è·å¾—æ›´å¥½çš„ç»“æœ
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ æˆ‘å°†åŸºäºä»¥ä¸‹Queryè¿›è¡Œç ”ç©¶ï¼š{sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # ä½¿ç”¨asyncio.gatherå¼‚æ­¥å¤„ç†å­æŸ¥è¯¢
        context = await asyncio.gather(
            *[
                self._process_sub_query_with_vectorstore(sub_query, filter)
                for sub_query in sub_queries
            ]
        )
        return context

    async def _get_context_by_web_search(self, query, scraped_data: list | None = None, query_domains: list | None = None):
        """
        é€šè¿‡æœç´¢Queryå¹¶æŠ“å–ç»“æœç”Ÿæˆç ”ç©¶ä»»åŠ¡çš„ä¸Šä¸‹æ–‡
        è¿”å›ï¼š
            context: ä¸Šä¸‹æ–‡åˆ—è¡¨
        """
        self.logger.info(f"å¼€å§‹ç½‘ç»œæœç´¢ï¼ŒQueryï¼š{query}")
        
        if scraped_data is None:
            scraped_data = []
        if query_domains is None:
            query_domains = []

        # ç”Ÿæˆå­æŸ¥è¯¢ï¼ŒåŒ…æ‹¬åŸå§‹Query
        sub_queries = await self.plan_research(query, query_domains)
        self.logger.info(f"ç”Ÿæˆçš„å­æŸ¥è¯¢ï¼š{sub_queries}")
        
        # å¦‚æœè¿™ä¸æ˜¯å­ç ”ç©¶è€…çš„ä¸€éƒ¨åˆ†ï¼Œæ·»åŠ åŸå§‹Queryä»¥è·å¾—æ›´å¥½çš„ç»“æœ
        if self.researcher.report_type != "subtopic_report":
            sub_queries.append(query)

        if self.researcher.verbose:
            await stream_output(
                "logs",
                "subqueries",
                f"ğŸ—‚ï¸ æˆ‘å°†åŸºäºä»¥ä¸‹Queryè¿›è¡Œç ”ç©¶ï¼š{sub_queries}...",
                self.researcher.websocket,
                True,
                sub_queries,
            )

        # ä½¿ç”¨asyncio.gatherå¼‚æ­¥å¤„ç†å­æŸ¥è¯¢
        try:
            context = await asyncio.gather(
                *[
                    self._process_sub_query(sub_query, scraped_data, query_domains)
                    for sub_query in sub_queries
                ]
            )
            self.logger.info(f"å·²ä» {len(context)} ä¸ªå­æŸ¥è¯¢ä¸­è·å–ä¸Šä¸‹æ–‡")
            # è¿‡æ»¤ç©ºç»“æœå¹¶åˆå¹¶ä¸Šä¸‹æ–‡
            context = [c for c in context if c]
            if context:
                combined_context = " ".join(context)
                self.logger.info(f"åˆå¹¶åä¸Šä¸‹æ–‡å¤§å°ï¼š{len(combined_context)}")
                return combined_context
            return []
        except Exception as e:
            self.logger.error(f"ç½‘ç»œæœç´¢æœŸé—´å‘ç”Ÿé”™è¯¯ï¼š{e}", exc_info=True)
            return []

    async def _process_sub_query(self, sub_query: str, scraped_data: list = [], query_domains: list = []):
        """æ¥æ”¶ä¸€ä¸ªå­æŸ¥è¯¢å¹¶æ ¹æ®å®ƒæŠ“å–URLå¹¶æ”¶é›†ä¸Šä¸‹æ–‡ã€‚"""
        if self.json_handler:
            self.json_handler.log_event("sub_query", {
                "query": sub_query,
                "scraped_data_size": len(scraped_data)
            })
        
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_research",
                f"\nğŸ” æ­£åœ¨å¯¹ '{sub_query}' è¿›è¡Œç ”ç©¶...",
                self.researcher.websocket,
            )

        try:
            if not scraped_data:
                scraped_data = await self._scrape_data_by_urls(sub_query, query_domains)
                self.logger.info(f"æŠ“å–æ•°æ®å¤§å°ï¼š{len(scraped_data)}")

            content = await self.researcher.context_manager.get_similar_content_by_query(sub_query, scraped_data)
            self.logger.info(f"å­æŸ¥è¯¢æ‰¾åˆ°çš„å†…å®¹å¤§å°ï¼š{len(str(content)) if content else 0} å­—ç¬¦")

            if not content and self.researcher.verbose:
                await stream_output(
                    "logs",
                    "subquery_context_not_found",
                    f"ğŸ¤· æ²¡æœ‰æ‰¾åˆ° '{sub_query}' çš„ç›¸å…³å†…å®¹...",
                    self.researcher.websocket,
                )
            if content:
                if self.json_handler:
                    self.json_handler.log_event("content_found", {
                        "sub_query": sub_query,
                        "content_size": len(content)
                    })
            return content
        except Exception as e:
            self.logger.error(f"å¤„ç†å­æŸ¥è¯¢ {sub_query} æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}", exc_info=True)
            return ""

    async def _process_sub_query_with_vectorstore(self, sub_query: str, filter: dict | None = None):
        """æ¥æ”¶ä¸€ä¸ªå­æŸ¥è¯¢å¹¶ä»ç”¨æˆ·æä¾›çš„VectorStoreä¸­æ”¶é›†ä¸Šä¸‹æ–‡

        å‚æ•°ï¼š
            sub_query (str): ä»åŸå§‹Queryç”Ÿæˆçš„å­æŸ¥è¯¢

        è¿”å›ï¼š
            str: ä»æœç´¢ä¸­è·å¾—çš„ä¸Šä¸‹æ–‡
        """
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "running_subquery_with_vectorstore_research",
                f"\nğŸ” æ­£åœ¨å¯¹ '{sub_query}' è¿›è¡Œç ”ç©¶...",
                self.researcher.websocket,
            )

        context = await self.researcher.context_manager.get_similar_content_by_query_with_vectorstore(sub_query, filter)

        return context

    async def _get_new_urls(self, url_set_input):
        """ä»ç»™å®šçš„URLé›†åˆä¸­è·å–æ–°URLã€‚
        å‚æ•°ï¼šurl_set_input (set[str])ï¼šä»ä¸­è·å–æ–°URLçš„URLé›†åˆ
        è¿”å›ï¼šlist[str]ï¼šç»™å®šURLé›†åˆä¸­çš„æ–°URL
        """

        new_urls = []
        for url in url_set_input:
            if url not in self.researcher.visited_urls:
                self.researcher.visited_urls.add(url)
                new_urls.append(url)
                if self.researcher.verbose:
                    await stream_output(
                        "logs",
                        "added_source_url",
                        f"âœ… å·²å°†æºURLæ·»åŠ åˆ°ç ”ç©¶ï¼š{url}\n",
                        self.researcher.websocket,
                        True,
                        url,
                    )

        return new_urls

    async def _search_relevant_source_urls(self, query, query_domains: list | None = None):
        new_search_urls = []
        if query_domains is None:
            query_domains = []

        # éå†æ‰€æœ‰æ£€ç´¢å™¨
        for retriever_class in self.researcher.retrievers:
            # ä½¿ç”¨å­æŸ¥è¯¢å®ä¾‹åŒ–æ£€ç´¢å™¨
            retriever = retriever_class(query, query_domains=query_domains)

            # ä½¿ç”¨å½“å‰æ£€ç´¢å™¨æ‰§è¡Œæœç´¢
            search_results = await asyncio.to_thread(
                retriever.search, max_results=self.researcher.cfg.max_search_results_per_query
            )

            # ä»æœç´¢ç»“æœä¸­æ”¶é›†æ–°URL
            search_urls = [url.get("href") for url in search_results]
            new_search_urls.extend(search_urls)

        # è·å–å”¯ä¸€URL
        new_search_urls = await self._get_new_urls(new_search_urls)
        random.shuffle(new_search_urls)

        return new_search_urls

    async def _scrape_data_by_urls(self, sub_query, query_domains: list | None = None):
        """
        åœ¨å¤šä¸ªæ£€ç´¢å™¨ä¸Šè¿è¡Œå­æŸ¥è¯¢å¹¶æŠ“å–ç”Ÿæˆçš„URLã€‚

        å‚æ•°ï¼š
            sub_query (str)ï¼šè¦æœç´¢çš„å­æŸ¥è¯¢ã€‚

        è¿”å›ï¼š
            listï¼šæŠ“å–å†…å®¹ç»“æœçš„åˆ—è¡¨ã€‚
        """
        if query_domains is None:
            query_domains = []

        new_search_urls = await self._search_relevant_source_urls(sub_query, query_domains)

        # å¦‚æœå¯ç”¨è¯¦ç»†æ¨¡å¼ï¼Œè®°å½•ç ”ç©¶è¿‡ç¨‹
        if self.researcher.verbose:
            await stream_output(
                "logs",
                "researching",
                f"ğŸ¤” æ­£åœ¨è·¨å¤šä¸ªæºç ”ç©¶ç›¸å…³ä¿¡æ¯...\n",
                self.researcher.websocket,
            )

        # æŠ“å–æ–°URL
        scraped_content = await self.researcher.scraper_manager.browse_urls(new_search_urls)

        if self.researcher.vector_store:
            self.researcher.vector_store.load(scraped_content)

        return scraped_content
